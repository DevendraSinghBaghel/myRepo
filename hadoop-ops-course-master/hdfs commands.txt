hadoop fs commands
===================

[hdfs@node1 ~]$ hadoop fs -ls /user
Found 1 items
drwxrwx---   - ambari-qa hdfs          0 2016-12-02 17:46 /user/ambari-qa
[hdfs@node1 ~]$ hadoop fs -mkdir /user/vagrant
[hdfs@node1 ~]$ hadoop fs -chown vagrant:vagrant /user/vagrant
[hdfs@node1 ~]$ hadoop fs -ls /user
Found 2 items
drwxrwx---   - ambari-qa hdfs             0 2016-12-02 17:46 /user/ambari-qa
drwxr-xr-x   - vagrant   vagrant          0 2016-12-22 13:57 /user/vagrant

hadoop fs -put /vagrant/data/salaries.csv salaries.csv -> copy from local to hdfs

hdfs fsck /user/vagrant/salaries.csv -> show the stat of teh file uploaded

hadoop fs -D dfs.blocksize=1m -put   /vagrant/data/salaries.csv  salaries2.csv
======================================================================================
hdfs fsck /user/vagrant/salaries2.csv

.Status: HEALTHY
 Total size:    16257213 B
 Total dirs:    0
 Total files:   1
 Total symlinks:                0
 *Total blocks (validated):      16 (avg. block size 1016075 B)*
 Minimally replicated blocks:   16 (100.0 %)
 Over-replicated blocks:        0 (0.0 %)
 Under-replicated blocks:       0 (0.0 %)
 Mis-replicated blocks:         0 (0.0 %)
 Default replication factor:    3
 Average block replication:     3.0
 Corrupt blocks:                0
 Missing replicas:              0 (0.0 %)
 Number of data-nodes:          3
 Number of racks:               1
FSCK ended at Thu Dec 22 14:07:34 UTC 2016 in 2 milliseconds


The filesystem under path '/user/vagrant/salaries2.csv' is HEALTHY
====================================================================================
hdfs fsck /user/vagrant/salaries2.csv -files -locations -blocks -> gives more info on the file

sudo find /hadoop/hdfs/  -name ='blk_1073741844*' -print

yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar
An example program must be given as the first argument.
Valid program names are:
  aggregatewordcount: An Aggregate based map/reduce program that counts the words in the input files.
  aggregatewordhist: An Aggregate based map/reduce program that computes the histogram of the words in the input files.
  bbp: A map/reduce program that uses Bailey-Borwein-Plouffe to compute exact digits of Pi.
  dbcount: An example job that count the pageview counts from a database.
  distbbp: A map/reduce program that uses a BBP-type formula to compute exact bits of Pi.
  grep: A map/reduce program that counts the matches of a regex in the input.
  join: A job that effects a join over sorted, equally partitioned datasets
  multifilewc: A job that counts words from several files.
  pentomino: A map/reduce tile laying program to find solutions to pentomino problems.
  pi: A map/reduce program that estimates Pi using a quasi-Monte Carlo method.
  randomtextwriter: A map/reduce program that writes 10GB of random textual data per node.
  randomwriter: A map/reduce program that writes 10GB of random data per node.
  secondarysort: An example defining a secondary sort to the reduce.
  sort: A map/reduce program that sorts the data written by the random writer.
  sudoku: A sudoku solver.
  teragen: Generate data for the terasort
  terasort: Run the terasort
  teravalidate: Checking results of terasort
  wordcount: A map/reduce program that counts the words in the input files.
  wordmean: A map/reduce program that counts the average length of the words in the input files.
  wordmedian: A map/reduce program that counts the median length of the words in the input files.
  wordstandarddeviation: A map/reduce program that counts the standard deviation of the length of the words in the input files.
 
yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar wordcount constitution.txt wc_output1