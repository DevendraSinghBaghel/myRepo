{"cells":[{"cell_type":"code","source":["import numpy as np\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.mllib.clustering import KMeans\n\n\n#generate random data RDD we need this package\nfrom pyspark.mllib.random import RandomRDDs"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["#let's generate random class data, add in a cluster center to random 2D points\n\n#use default num of partitions, or use a definte number to make it so that the union\n#  will have samples across clusters\nc1_v=RandomRDDs.normalVectorRDD(sc,20,2,numPartitions=2,seed=1L).map(lambda v:np.add([1,5],v))\nc2_v=RandomRDDs.normalVectorRDD(sc,16,2,numPartitions=2,seed=2L).map(lambda v:np.add([5,1],v))\nc3_v=RandomRDDs.normalVectorRDD(sc,12,2,numPartitions=2,seed=3L).map(lambda v:np.add([4,6],v))"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["#concatenate 2 RDDs with  .union(other) function\nc12    =c1_v.union(c2_v)\nmy_data=c12.union(c3_v)   #this now has all points, as RDD\n"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["my_data.stats()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["my_kmmodel = KMeans.train(my_data,k=3,\n               maxIterations=20,runs=1,\n               initializationMode='k-means||',seed=10L)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["dir(my_kmmodel)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["my_kmmodel.centers\nmy_kmmodel.clusterCenters"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["my_kmmodel"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["#try: help(KMeans.train)  to see parameter options\n#k is the number of desired clusters.\n#maxIterations is the maximum number of iterations to run.\n#initializationMode specifies either random initialization or initialization via k-means||.\n#runs is the number of times to run the k-means algorithm (k-means is not guaranteed to find a globally optimal solution, and when run multiple times on a given dataset, the algorithm returns the best clustering result).\n#initializationSteps determines the number of steps in the k-means|| algorithm.\n#epsilon determines the distance threshold within which we consider k-means to have converged.\n \n\n#type dir(my_kmmodel) to see functions available on the cluster results object\n\n#The computeCost function might not be available on your cloudera vm,\n#  spark mlllib, it computes the Sum Squared Error: my_kmmodel.computeCost(my_data)  \n\n#This does the same thing as computeCost, and gives an example of coding a metric\n#get sse of a point to the center of the cluster it's assigned to"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["def getsse(point):\n    this_center = my_kmmodel.centers[my_kmmodel.predict(point)]\n           #for this point get it's clustercenter\n    return (sum([x**2 for x in (point - this_center)])) \n\n\nmy_sse=my_data.map(getsse).collect()  #collect list of sse of each pt to its center\n#print my_sse\nprint np.array(my_sse).mean() "],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":11}],"metadata":{"name":"Coursera_BigData_ML_W4","notebookId":3361934129695124},"nbformat":4,"nbformat_minor":0}
