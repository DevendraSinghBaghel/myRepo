{"cells":[{"cell_type":"code","source":["import numpy as np\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.mllib.regression import LabeledPoint\nfrom pyspark.sql import SQLContext,Row"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["#outlook,temperature,humidity,windy,play, copied from Weka's data example\nrawdata=[\n['sunny',85,85,'FALSE',0,1],\n['sunny',80,90,'TRUE',0,1],\n['overcast',83,86,'FALSE',1,1],\n['rainy',70,96,'FALSE',1,1],\n['rainy',68,80,'FALSE',1,1],\n['rainy',65,70,'TRUE',0,1],\n['overcast',64,65,'TRUE',1,1],\n['sunny',72,95,'FALSE',0,1],\n['sunny',69,70,'FALSE',1,1],\n['rainy',75,80,'FALSE',1,1],\n['sunny',75,70,'TRUE',1,1],\n['overcast',72,90,'TRUE',1,1],\n['overcast',81,75,'FALSE',1,1],\n['rainy',71,91,'TRUE',0,1]\n]\n\n"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["data_df=sqlContext.createDataFrame(rawdata,\n   ['outlook','temp','humid','windy','play','mydummy'])"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["#transform categoricals into indicator variables\nout2index={'sunny':[1,0,0],'overcast':[0,1,0],'rainy':[0,0,1]}"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["def newrow(dfrow):\n    outrow = list(out2index.get((dfrow[0])))  #get dictionary entry for outlook\n    outrow.append(dfrow[1])   #temp\n    outrow.append(dfrow[2])   #humidity\n    if dfrow[3]=='TRUE':      #windy\n        outrow.append(1)\n    else:\n        outrow.append(0)\n    outrow.append(dfrow[5])\n    return (LabeledPoint(dfrow[4],outrow))\ndatax_rdd = data_df.map(newrow)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["datax_rdd.collect()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["from pyspark.mllib.classification import NaiveBayes"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["#execute model, it can go in a single pass\nmy_nbmodel = NaiveBayes.train(datax_rdd)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["#Some info on model \nprint my_nbmodel\n"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["#some checks,get some of training data and test it:\ndatax_col=datax_rdd.collect()   #if datax_rdd was big, use sample or take\n\ntrainset_pred =[]\nfor x in datax_col:\n    trainset_pred.append(my_nbmodel.predict(x.features))\n\nprint trainset_pred"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["#to see class conditionals\n#you might have to install scipy\nimport scipy\nprint 'Class Cond Probabilities, ie p(attr|class= 0 or 1) '\nprint scipy.exp(my_nbmodel.theta)\nprint scipy.exp(my_nbmodel.pi)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["#get a confusion matrix\n#the row is the true class label 0 or 1, columns are predicted label\n#\nnb_cf_mat=np.zeros([2,2])  #num of classes\nfor pnt in datax_col:\n    predctn = my_nbmodel.predict(np.array(pnt.features))\n    nb_cf_mat[pnt.label][predctn]+=1\n\ncorrcnt=0\nprint nb_cf_mat\n\nfor i in range(2):\n    corrcnt+=nb_cf_mat[i][i]\nnb_per_corr=corrcnt/nb_cf_mat.sum()\nprint 'Naive Bayes: Conf.Mat. and Per Corr'\nprint nb_cf_mat\nprint nb_per_corr"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# ---------------- now try decision tree ------------\nfrom pyspark.mllib.tree import DecisionTree\ndt_model = DecisionTree.trainClassifier(datax_rdd,2,{},impurity='entropy',\n          maxDepth=3,maxBins=32, minInstancesPerNode=2)  "],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["#maxDepth and maxBins\n#{} could be categorical feature list,\n# To do regression, have no numclasses,and use trainRegression function\nprint dt_model.toDebugString()\n\n#results in this:\n#DecisionTreeModel classifier of depth 3 with 9 nodes\n#  If (feature 1 <= 0.0)\n#   If (feature 4 <= 80.0)\n#    If (feature 3 <= 68.0)\n#     Predict: 0.0\n#    Else (feature 3 > 68.0)\n#     Predict: 1.0\n#   Else (feature 4 > 80.0)\n#    If (feature 0 <= 0.0)\n#     Predict: 0.0\n#    Else (feature 0 > 0.0)\n#     Predict: 0.0\n#  Else (feature 1 > 0.0)\n#   Predict: 1.0\n\n#notice number of nodes are the predict (leaf nodes) and the ifs\n           \n"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["#some checks,get some of training data and test it:\ndatax_col=datax_rdd.collect()   #if datax_rdd was big, use sample or take\n\n#redo the conf. matrix code (it would be more efficient to pass a model)\ndt_cf_mat=np.zeros([2,2])  #num of classes\nfor pnt in datax_col:\n    predctn = dt_model.predict(np.array(pnt.features))\n    dt_cf_mat[pnt.label][predctn]+=1\ncorrcnt=0\nfor i in range(2): \n    corrcnt+=dt_cf_mat[i][i]\ndt_per_corr=corrcnt/dt_cf_mat.sum()\nprint 'Decision Tree: Conf.Mat. and Per Corr'\nprint dt_cf_mat\nprint dt_per_corr\n\n#maxdepth 5\n# print cf_mat\n#[[ 5.  0.]\n# [ 2.  7.]]\n#>>> print per_corr\n#0.857142857143\n\n\n#maxdepty 3 sis same ass 5\n#maxdepth 2 gives me a core dump!\n"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["newpoint  = np.array([1,0,0,68,79,0,1])"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["my_nbmodel.predict(newpoint)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["dt_model.predict(newpoint)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":19}],"metadata":{"name":"Coursera_BigData_ML_W2","notebookId":92842611643765},"nbformat":4,"nbformat_minor":0}
