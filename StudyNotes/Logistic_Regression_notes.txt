Logistic Regression
---------------------

The Logistic Response Function is
used to produce a number between 0 and 1.

The logistic response function always
takes values between 0 and 1, which makes sense,
since it equals a probability.
A positive coefficient value for a variable
increases the linear regression piece,
which increases the probability that y = 1,
or increases the probability of poor care.
On the other hand, a negative coefficient value
for a variable decreases the linear regression piece,
which in turn decreases the probability that y = 1,
or increases the probability of good care.

The coefficients, or betas, are selected
to predict a high probability for the actual poor care cases,
and to predict a low probability for the actual good care cases.
Another useful way to think about the logistic response
function is in terms of Odds, like in gambling.
The Odds are the probability of 1
divided by the probability of 0.
The Odds are greater than 1 if 1 is more likely, and less than 1
if 0 is more likely.
The Odds are equal to 1 if the outcomes are equally likely.

If you substitute the Logistic Response Function
for the probabilities in the Odds
equation on the previous slide, you
can see that the Odds are equal to "e" raised
to the power of the linear regression equation.
By taking the log of both sides, the log(Odds),
or what we call the Logit, looks exactly
like the linear regression equation.

A positive beta value increases the Logit,
which in turn increases the Odds of 1.
A negative beta value decreases the Logit,
which in turn, decreases the Odds of one.

In a classification problem, a standard baseline method
is to just predict the most frequent outcome
for all observations.

We see here that the coefficients for OfficeVisits
and Narcotics are both positive, which
means that higher values in these two variables
are indicative of poor care as we
suspected from looking at the data.

The last thing we want to look at in the output
is the AIC value.
This is a measure of the quality of the model
and is like Adjusted R-squared in that it accounts
for the number of variables used compared
to the number of observations.
Unfortunately, it can only be compared
between models on the same data set.
But it provides a means for model selection.
The preferred model is the one with the minimum AIC.

There are two types of errors that a model can make --
ones where you predict 1, or poor care,
but the actual outcome is 0, and ones where you predict 0,
or good care, but the actual outcome is 1.
If we pick a large threshold value t,
then we will predict poor care rarely,
since the probability of poor care
has to be really large to be greater than the threshold.
This means that we will make more errors where
we say good care, but it's actually poor care.
This approach would detect the patients receiving the worst
care and prioritize them for intervention.
On the other hand, if the threshold value, t, is small,
we predict poor care frequently, and we predict good care
rarely.

This means that we will make more errors where
we say poor care, but it's actually good care.
This approach would detect all patients
who might be receiving poor care.
Some decision-makers often have a preference
for one type of error over the other,
which should influence the threshold value they pick.
If there's no preference between the errors,
the right threshold to select is t = 0.5,
since it just predicts the most likely outcome.
To make this discussion a little more quantitative,
we use what's called a confusion matrix or classification.

This compares the actual outcomes
to the predicted outcomes.
The rows are labeled with the actual outcome,
and the columns are labeled with the predicted outcome.
Each entry of the table gives the number of data
observations that fall into that category.
So the number of true negatives, or TN,
is the number of observations that are actually
good care and for which we predict good care.
The true positives, or TP, is the number
of observations that are actually
poor care and for which we predict poor care.
These are the two types that we get correct.
The false positives, or FP, are the number of data points
for which we predict poor care, but they're actually good care.
And the false negatives, or FN, are the number of data points
for which we predict good care, but they're actually poor care.

We can compute two outcome measures
that help us determine what types of errors we are making.
They're called sensitivity and specificity.
Sensitivity is equal to the true positives
divided by the true positives plus the false negatives,
and measures the percentage of actual poor care cases
that we classify correctly.
This is often called the true positive rate.
Specificity is equal to the true negatives
divided by the true negatives plus the false positives,
and measures the percentage of actual good care cases
that we classify correctly.
This is often called the true negative rate.
A model with a higher threshold will have a lower sensitivity
and a higher specificity.
A model with a lower threshold will have a higher sensitivity
and a lower specificity.

Roc Curve
=========

A Receiver Operator Characteristic curve,
or ROC curve, can help you decide
which value of the threshold is best.

The sensitivity, or true positive rate of the model,
is shown on the y-axis.
And the false positive rate, or 1 minus the specificity,
is given on the x-axis.
The line shows how these two outcome measures
vary with different threshold values.

The ROC curve always starts at the point (0, 0).
This corresponds to a threshold value of 1.
If you have a threshold of 1, you
will not catch any poor care cases,
or have a sensitivity of 0.
But you will correctly label of all the good care
cases, meaning you have a false positive rate of 0.

The ROC curve always ends at the point (1,1),
which corresponds to a threshold value of 0.
If you have a threshold of 0, you'll
catch all of the poor care cases,
or have a sensitivity of 1, but you'll
label all of the good care cases as poor care cases
too, meaning you have a false positive rate of 1.
The threshold decreases as you move from (0,0) to (1,1).

The ROC curve captures all thresholds simultaneously.
The higher the threshold, or closer to (0, 0),
the higher the specificity and the lower the sensitivity.
The lower the threshold, or closer to (1,1),
the higher the sensitivity and lower the specificity.
So which threshold value should you pick?
You should select the best threshold
for the trade-off you want to make.
If you're more concerned with having a high specificity
or low false positive rate, pick the threshold
that maximizes the true positive rate
while keeping the false positive rate really low.

On the other hand, if you're more concerned
with having a high sensitivity or high true positive rate,
pick a threshold that minimizes the false positive rate
but has a very high true positive rate.


Interperting the Model
=======================
Multicollinearity occurs when the various independent
variables are correlated, and this
might confuse the coefficients-- the betas-- in the model.
So tests to address that involve checking the correlations
of independent variables.
If they are excessively high, this
would mean that there might be multicollinearity,
and you have to potentially revisit the model,
as well as whether the signs of the coefficients make sense.
Is the coefficient beta positive or negative?
If it agrees with intuition, then multicollinearity
has not been a problem, but if intuition suggests
a different sign, this might be a sign of multicollinearity.

The Area Under the Curve, or AUC for short, is used to interpert a logistic regression model
So the Area Under the Curve shows an absolute measure
of quality of prediction-- in this particular case, 77.5%,
which means that, given that the perfect score is 100%,
so this is like a B, whereas, as we'll see later,
a 50% score, which is pure guessing,
is a 50% rate of success.
So the area under the curve gives an absolute measure
of quality, and it's less affected by various benchmarks.
So it illustrates how accurate the model
is on a more absolute sense.

TP=True Postive TN=True Negative
FP=False Postive FN=False Negative
N=No:of Observations

Accuracy = TP+TN/N 

overall error rate = FP+FN/N

sensitivity = TP/TP+FN 
False Negative error rate = FN/TP+FN

specificity = TN/TN+FP 
False Positive error rate = FP/TN+FP 



