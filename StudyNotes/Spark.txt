
A note about DataFrames and queries
====================================

When you use DataFrames or Spark SQL, you are building up a query plan. Each transformation you apply to a DataFrame adds some information to the query plan. When you finally call an action, which triggers execution of your Spark job, several things happen:

Spark's Catalyst optimizer analyzes the query plan (called an unoptimized logical query plan) and attempts to optimize it. Optimizations include (but aren't limited to) rearranging and combining filter() operations for efficiency, converting Decimal operations to more efficient long integer operations, and pushing some operations down into the data source (e.g., a filter() operation might be translated to a SQL WHERE clause, if the data source is a traditional SQL RDBMS). The result of this optimization phase is an optimized logical plan.

Once Catalyst has an optimized logical plan, it then constructs multiple physical plans from it. Specifically, it implements the query in terms of lower level Spark RDD operations.

Catalyst chooses which physical plan to use via cost optimization. That is, it determines which physical plan is the most efficient (or least expensive), and uses that one.

Finally, once the physical RDD execution plan is established, Spark actually executes the job.

You can examine the query plan using the explain() function on a DataFrame. By default, explain() only shows you the final physical plan; however, if you pass it an argument of True, it will show you all phases.

Transformations
================

select() is a transformation. It returns a new DataFrame that captures both the previous DataFrame and the operation to add to the query (select, in this case). 

filter() method is a transformation operation that creates a new DataFrame from the input DataFrame, keeping only values that match the filter expression.


Actions
========

 Action operations cause Spark to perform the (lazy) transformation operations that are required to compute the values returned by the action.

collect - will dump all the results

show - will display in tabular format, 20 rows at a time

subDF.show(n=30, truncate=False) - we can increase the display limit in show

display() - still more beautiful way to show results

count() - is an action operation, Each task counts the entries in its partition and sends the result to your SparkContext, which adds up all of the counts. 

