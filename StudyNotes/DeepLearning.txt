DeepLearning
==============

-DeepLearning uses very powerful NNs for predictions

-DeepLearning facilitates interactions between different input features 

-Inputlayer,Hiddenlayer,Outputlayer are three different layers in a NN

-more nodes in the Hiddenlayer will facilitates the NN to capture more interactions between
 input

- The firing of a NN classifier, or activation as its commonly called, produces a score.

-A neural net can be viewed as the result of spinning classifiers together in a layered web.
 This is because each node in the hidden and output layers has its own classifier.
 
- A set of inputs is passed to the first hidden layer,
  the activations from that layer are passed to the next layer and so on,
  until you reach the output layer,where the results of the classification are determined by the scores at each node.
  This happens for each set of inputs.
  
- This series of events starting from the input where each activation is sent to the next layer, and then the next, all the way to the output,
  is known as forward propagation, or forward prop.Forward prop is a neural net's way of classifying a set of inputs.

- Each node has the same classifier, and none of them fire randomly;
  if you repeat an input, you get the same output.
  So if every node in the hidden layer received the same input,
  why didn’t they all fire out the same value?
  The reason is that each set of inputs is modified by unique weights and biases.
  
-You may have guessed that the prediction accuracy of a neural net depends on its weights and biases.
We want that accuracy to be high,
meaning we want the neural net to predict a value that is as close to the actual output as possible,
every single time.
The process of improving a neural net’s accuracy is called training

- to train the net, the output from forward prop is compared to the output that is known to be correct,
and the cost is the difference of the two.
The point of training is to make that cost as small as possible, across millions of training examples.
To do this, the net tweaks the weights and biases step by step
until the prediction closely matches the correct output.
Once trained well, a neural net has the potential to make accurate predictions each time.

- let’s say that a net had to decide whether or not an image contained a human face. A
deep net would first use edges to detect different parts of the face – the lips, nose, eyes,
ears, and so on – and would then combine the results together to form the whole face.
This important feature – using simpler patterns as building blocks to detect complex patterns

- if you’re interested in unsupervised learning – that is, you want to extract patterns
from a set of unlabelled data – then your best bet is to use either a Restricted Boltzmann
Machine, or an autoencoder.

-For text processing tasks like sentiment analysis, parsing, and named entity recognition – use
a Recurrent Net or a Recursive Neural Tensor Network, which we’ll refer to as an RNTN.

- For any language model that operates on the character level, use a Recurrent Net.

- For image recognition, use a Deep Belief Network or a Convolutional Net.

- For object recognition, use a Convolutional Net or an RNTN.

- Finally, for speech recognition, use a Recurrent Net.

- In general, Deep Belief Networks and Multilayer Perceptrons with rectified linear units – also
known as RELU – are both good choices for classification. 

- For time series analysis, it’s best to use a Recurrent Net.

- Deep Nets are the current state of the art in pattern recognition

- Well as it turns out, when you try to train deep nets
with a method called backpropagation, you run into a fundamental problem called the
vanishing gradient, or sometimes the exploding gradient. When that happens, training takes
too long and the accuracy really suffers. 

- Until 2006, there was no way to accurately
train deep nets due to a fundamental problem with the training process: the vanishing gradient.
Let’s think of a gradient like a slope, and the training process like a rock rolling
down that slope. A rock will roll quickly down a steep slope but will barely move at
all on a flat surface. The same is true with the gradient of a deep net. When the gradient
is large, the net will train quickly. When the gradient is small, the net will train
slowly.

-  The gradients are much smaller in the
earlier layers. As a result, the early layers of the network are the slowest to train. But
this is a fundamental problem! The early layers are responsible for detecting the simple patterns
and the building blocks – when it came to facial recognition, the early layers detected
the edges which were combined to form facial features later in the network. And if the
early layers get it wrong, the result built up by the net will be wrong as well.

- The process used for training a neural net is called back-propagation or back-prop. We
saw before that forward prop starts with the inputs and works forward; back-prop does the
reverse, calculating the gradient from right to left. For example, here are 5 gradients,
4 weight and 1 bias. It starts with the left and works back through the layers, like so.
Each time it calculates a gradient, it uses all the previous gradients up to that point.
So, lets start with that node. That edge uses the gradient at that node. And the next. So
far things are simple. As you keep going back, things get a bit more complex 

- Restricted Boltzmann Machine is a method that can automatically find patterns in our data by reconstructing
the input. 